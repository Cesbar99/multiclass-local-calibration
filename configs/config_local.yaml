defaults:
  - _self_
  - models: mlp
  - dataset: synthetic

# Global parameters
seed: 42
use_wandb: True
resume_training: False
offline: False
device: cuda
cuda_device: 0

# main commands
pretrain: False
test: False
calibrate: True
exp_name: 
data: 'synthetic'
calibration_method: "local_calibration"

# synthetic data model
#init_logits_scaling: 1.

# weight and bias
wandb_entity: cesare-barbera-university-of-trento
wandb_project: calibration-experiments
wandb_id:
use_wand: True

# testing and calibration metrics
n_bins_calibration_metrics: 15
save_path_calibration_plots: /home/barbera/calibration/localibration/plots/

# default behaviour for batch size
batch_size_map:
  pretraining: 32
  calibration: 512



